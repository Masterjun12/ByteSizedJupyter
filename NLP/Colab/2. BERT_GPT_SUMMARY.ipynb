{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c632bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 11:42:48.524026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4033 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:3b:00.0, compute capability: 8.0\n",
      "2023-12-20 11:42:48.526485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 4033 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:5e:00.0, compute capability: 8.0\n",
      "2023-12-20 11:42:48.528649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 4033 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n",
      "2023-12-20 11:42:48.530811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 4033 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "config = tf1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "session = tf1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8376e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rouge import Rouge\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9d624",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "947f776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/a202192010/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/a202192010/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 불용어(stopwords) 목록 로드\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "14c0dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 전처리 함수\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def preprocess_news_article(news_article):\n",
    "    # 문장 토큰화\n",
    "    sentences = sent_tokenize(news_article)\n",
    "\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # 문장 토큰화\n",
    "        tokens = word_tokenize(sentence)\n",
    "\n",
    "        # 특수문자 및 숫자 제거\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha() or word not in string.punctuation]\n",
    "\n",
    "        # 토큰들을 다시 문장으로 결합\n",
    "        processed_text = ' '.join(tokens)\n",
    "\n",
    "        processed_sentences.append(processed_text)\n",
    "\n",
    "    return processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3ef355eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 뉴스 기사\n",
    "\n",
    "NEWS_ARTICLE = \"The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e4eac661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 기사: ['the daman and diu administration on wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.the union territory s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate rakshabandhan at workplace', 'it has been decided to celebrate the festival of rakshabandhan on august 7', 'in this connection all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues', 'the order issued on august 1 by gurpreet singh deputy secretary personnel had said.to ensure that no one skipped office an attendance report was to be sent to the government the next evening.the two notifications', 'one mandating the celebration of rakshabandhan left and the other withdrawing the mandate right', 'were issued by the daman and diu administration a day apart', 'the circular was withdrawn through a one-line order issued late in the evening by the ut s department of personnel and administrative reforms', 'the circular is ridiculous', 'there are sensitivities involved', 'how can the government dictate who i should tie rakhi to', 'we should maintain the professionalism of a workplace', 'an official told hindustan times earlier in the day', 'she refused to be identified.the notice was issued on daman and diu administrator and former gujarat home minister praful kodabhai patel s direction sources said.rakshabandhan a celebration of the bond between brothers and sisters is one of several hindu festivities and rituals that are no longer confined of private family affairs but have become tools to push politic al ideologies.in 2014 the year bjp stormed to power at the centre rashtriya swayamsevak sangh rss chief mohan bhagwat said the festival had national significance', 'and should be celebrated widely to protect hindu culture and live by the values enshrined in it', 'the rss is the ideological parent of the ruling bjp.last year women ministers in the modi government went to the border areas to celebrate the festival with soldiers', 'a year before all cabinet ministers were asked to go to their constituencies for the festival']\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 기사 전처리\n",
    "processed_article = preprocess_news_article(NEWS_ARTICLE)\n",
    "print(\"전처리된 기사:\", processed_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64a16f",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ba71b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9696cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 65.7kB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:24<00:00, 18.2MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.82kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 601kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 680kB/s]\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 및 토크나이저 로드\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f3a82781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  627,\n",
       "  9656,\n",
       "  260,\n",
       "  8,\n",
       "  2269,\n",
       "  257,\n",
       "  942,\n",
       "  15,\n",
       "  18862,\n",
       "  46836,\n",
       "  13666,\n",
       "  10,\n",
       "  18629,\n",
       "  14,\n",
       "  553,\n",
       "  390,\n",
       "  813,\n",
       "  7,\n",
       "  3318,\n",
       "  910,\n",
       "  7352,\n",
       "  354,\n",
       "  15,\n",
       "  2943,\n",
       "  4025,\n",
       "  71,\n",
       "  5,\n",
       "  645,\n",
       "  7544,\n",
       "  10,\n",
       "  10279,\n",
       "  31,\n",
       "  1321,\n",
       "  8,\n",
       "  21,\n",
       "  12256,\n",
       "  4102,\n",
       "  15,\n",
       "  592,\n",
       "  433,\n",
       "  4,\n",
       "  627,\n",
       "  2918,\n",
       "  4284,\n",
       "  579,\n",
       "  942,\n",
       "  21,\n",
       "  1654,\n",
       "  7,\n",
       "  11298,\n",
       "  624,\n",
       "  706,\n",
       "  722,\n",
       "  9,\n",
       "  10392,\n",
       "  5,\n",
       "  18629,\n",
       "  14,\n",
       "  156,\n",
       "  24,\n",
       "  25707,\n",
       "  13,\n",
       "  63,\n",
       "  813,\n",
       "  7,\n",
       "  3379,\n",
       "  910,\n",
       "  677,\n",
       "  1193,\n",
       "  31865,\n",
       "  4134,\n",
       "  23,\n",
       "  7637,\n",
       "  2],\n",
       " [0,\n",
       "  405,\n",
       "  34,\n",
       "  57,\n",
       "  1276,\n",
       "  7,\n",
       "  3379,\n",
       "  5,\n",
       "  3241,\n",
       "  9,\n",
       "  910,\n",
       "  677,\n",
       "  1193,\n",
       "  31865,\n",
       "  4134,\n",
       "  15,\n",
       "  26713,\n",
       "  4193,\n",
       "  262,\n",
       "  2],\n",
       " [0,\n",
       "  179,\n",
       "  42,\n",
       "  2748,\n",
       "  70,\n",
       "  4088,\n",
       "  73,\n",
       "  6522,\n",
       "  5658,\n",
       "  1091,\n",
       "  490,\n",
       "  8,\n",
       "  3379,\n",
       "  5,\n",
       "  3241,\n",
       "  14332,\n",
       "  23,\n",
       "  10,\n",
       "  10686,\n",
       "  86,\n",
       "  26134,\n",
       "  70,\n",
       "  5,\n",
       "  6429,\n",
       "  813,\n",
       "  5658,\n",
       "  3318,\n",
       "  910,\n",
       "  7352,\n",
       "  354,\n",
       "  7,\n",
       "  49,\n",
       "  4025,\n",
       "  2],\n",
       " [0,\n",
       "  627,\n",
       "  645,\n",
       "  1167,\n",
       "  15,\n",
       "  26713,\n",
       "  4193,\n",
       "  112,\n",
       "  30,\n",
       "  821,\n",
       "  710,\n",
       "  642,\n",
       "  14081,\n",
       "  7884,\n",
       "  298,\n",
       "  3193,\n",
       "  2971,\n",
       "  3775,\n",
       "  56,\n",
       "  26,\n",
       "  4,\n",
       "  560,\n",
       "  1306,\n",
       "  14,\n",
       "  117,\n",
       "  65,\n",
       "  22904,\n",
       "  558,\n",
       "  41,\n",
       "  6856,\n",
       "  266,\n",
       "  21,\n",
       "  7,\n",
       "  28,\n",
       "  1051,\n",
       "  7,\n",
       "  5,\n",
       "  168,\n",
       "  5,\n",
       "  220,\n",
       "  1559,\n",
       "  4,\n",
       "  627,\n",
       "  80,\n",
       "  16926,\n",
       "  2],\n",
       " [0,\n",
       "  1264,\n",
       "  17047,\n",
       "  1295,\n",
       "  5,\n",
       "  4821,\n",
       "  9,\n",
       "  910,\n",
       "  677,\n",
       "  1193,\n",
       "  31865,\n",
       "  4134,\n",
       "  314,\n",
       "  8,\n",
       "  5,\n",
       "  97,\n",
       "  20049,\n",
       "  5,\n",
       "  7184,\n",
       "  235,\n",
       "  2],\n",
       " [0, 17341, 1167, 30, 5, 9656, 260, 8, 2269, 257, 942, 10, 183, 4102, 2],\n",
       " [0,\n",
       "  627,\n",
       "  18629,\n",
       "  21,\n",
       "  13375,\n",
       "  149,\n",
       "  10,\n",
       "  65,\n",
       "  12,\n",
       "  1902,\n",
       "  645,\n",
       "  1167,\n",
       "  628,\n",
       "  11,\n",
       "  5,\n",
       "  1559,\n",
       "  30,\n",
       "  5,\n",
       "  16080,\n",
       "  579,\n",
       "  1494,\n",
       "  9,\n",
       "  3775,\n",
       "  8,\n",
       "  6833,\n",
       "  4907,\n",
       "  2],\n",
       " [0, 627, 18629, 16, 10861, 2],\n",
       " [0, 8585, 32, 28195, 34201, 963, 2],\n",
       " [0, 9178, 64, 5, 168, 22203, 54, 939, 197, 3318, 910, 677, 3592, 7, 2],\n",
       " [0, 1694, 197, 3014, 5, 23210, 9, 10, 7637, 2],\n",
       " [0, 260, 781, 174, 16175, 4193, 260, 498, 656, 11, 5, 183, 2],\n",
       " [0,\n",
       "  8877,\n",
       "  3179,\n",
       "  7,\n",
       "  28,\n",
       "  2006,\n",
       "  4,\n",
       "  627,\n",
       "  3120,\n",
       "  21,\n",
       "  1167,\n",
       "  15,\n",
       "  9656,\n",
       "  260,\n",
       "  8,\n",
       "  2269,\n",
       "  257,\n",
       "  11417,\n",
       "  8,\n",
       "  320,\n",
       "  11843,\n",
       "  11978,\n",
       "  415,\n",
       "  184,\n",
       "  1269,\n",
       "  21950,\n",
       "  2650,\n",
       "  449,\n",
       "  1630,\n",
       "  873,\n",
       "  30279,\n",
       "  181,\n",
       "  26591,\n",
       "  579,\n",
       "  2698,\n",
       "  1715,\n",
       "  26,\n",
       "  4,\n",
       "  763,\n",
       "  2258,\n",
       "  13899,\n",
       "  463,\n",
       "  4134,\n",
       "  10,\n",
       "  4821,\n",
       "  9,\n",
       "  5,\n",
       "  2175,\n",
       "  227,\n",
       "  5396,\n",
       "  8,\n",
       "  7502,\n",
       "  16,\n",
       "  65,\n",
       "  9,\n",
       "  484,\n",
       "  1368,\n",
       "  36232,\n",
       "  15972,\n",
       "  8,\n",
       "  27830,\n",
       "  14,\n",
       "  32,\n",
       "  117,\n",
       "  1181,\n",
       "  18687,\n",
       "  9,\n",
       "  940,\n",
       "  284,\n",
       "  5185,\n",
       "  53,\n",
       "  33,\n",
       "  555,\n",
       "  3270,\n",
       "  7,\n",
       "  1920,\n",
       "  19599,\n",
       "  1076,\n",
       "  35380,\n",
       "  4,\n",
       "  179,\n",
       "  777,\n",
       "  5,\n",
       "  76,\n",
       "  741,\n",
       "  34897,\n",
       "  16965,\n",
       "  7,\n",
       "  476,\n",
       "  23,\n",
       "  5,\n",
       "  2100,\n",
       "  21563,\n",
       "  21237,\n",
       "  2636,\n",
       "  17980,\n",
       "  424,\n",
       "  1090,\n",
       "  705,\n",
       "  677,\n",
       "  15610,\n",
       "  4147,\n",
       "  910,\n",
       "  7485,\n",
       "  834,\n",
       "  475,\n",
       "  14669,\n",
       "  741,\n",
       "  298,\n",
       "  1073,\n",
       "  24749,\n",
       "  26,\n",
       "  5,\n",
       "  3241,\n",
       "  56,\n",
       "  632,\n",
       "  11382,\n",
       "  2],\n",
       " [0,\n",
       "  463,\n",
       "  197,\n",
       "  28,\n",
       "  5417,\n",
       "  3924,\n",
       "  7,\n",
       "  1744,\n",
       "  1368,\n",
       "  36232,\n",
       "  2040,\n",
       "  8,\n",
       "  697,\n",
       "  30,\n",
       "  5,\n",
       "  3266,\n",
       "  29376,\n",
       "  6158,\n",
       "  11,\n",
       "  24,\n",
       "  2],\n",
       " [0,\n",
       "  627,\n",
       "  910,\n",
       "  7485,\n",
       "  16,\n",
       "  5,\n",
       "  18273,\n",
       "  4095,\n",
       "  9,\n",
       "  5,\n",
       "  2255,\n",
       "  741,\n",
       "  34897,\n",
       "  4,\n",
       "  13751,\n",
       "  76,\n",
       "  390,\n",
       "  5118,\n",
       "  11,\n",
       "  5,\n",
       "  11134,\n",
       "  118,\n",
       "  168,\n",
       "  439,\n",
       "  7,\n",
       "  5,\n",
       "  1424,\n",
       "  911,\n",
       "  7,\n",
       "  3379,\n",
       "  5,\n",
       "  3241,\n",
       "  19,\n",
       "  3878,\n",
       "  2],\n",
       " [0,\n",
       "  102,\n",
       "  76,\n",
       "  137,\n",
       "  70,\n",
       "  5892,\n",
       "  5118,\n",
       "  58,\n",
       "  553,\n",
       "  7,\n",
       "  213,\n",
       "  7,\n",
       "  49,\n",
       "  22449,\n",
       "  13,\n",
       "  5,\n",
       "  3241,\n",
       "  2]]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저\n",
    "tokenized_sentences = [tokenizer.encode(sent, add_special_tokens=True) for sent in processed_article]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "27fcf56b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  627, 9656,  ...,    0,    0,    0],\n",
       "        [   0,  405,   34,  ...,    0,    0,    0],\n",
       "        [   0,  179,   42,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,  463,  197,  ...,    0,    0,    0],\n",
       "        [   0,  627,  910,  ...,    0,    0,    0],\n",
       "        [   0,  102,   76,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 길이 구하기\n",
    "max_len = 0\n",
    "for i in tokenized_sentences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "# 패딩된 문장들\n",
    "padded_sentences = []\n",
    "for i in tokenized_sentences:\n",
    "    # 최대 길이에 도달할 때까지 0으로 패딩\n",
    "    while len(i) < max_len:\n",
    "        i.append(0)\n",
    "    padded_sentences.append(i)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "input_ids = torch.tensor(padded_sentences)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "b6d107b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 118])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b5ea4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9a8fb432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = bert_model.config.vocab_size # 모델의 vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d2f5df66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  627, 9656,  ...,    0,    0,    0],\n",
       "        [   0,  405,   34,  ...,    0,    0,    0],\n",
       "        [   0,  179,   42,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,  463,  197,  ...,    0,    0,    0],\n",
       "        [   0,  627,  910,  ...,    0,    0,    0],\n",
       "        [   0,  102,   76,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30500\n",
    "max_index = torch.max(input_ids)\n",
    "\n",
    "# input_ids 내의 모든 값을 어휘 크기 이하로 제한\n",
    "input_ids[input_ids > vocab_size] = vocab_size\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d63acbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 118])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "91c0be5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30500)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = torch.max(input_ids)\n",
    "max_index # 최댓값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "6021f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 출력\n",
    "with torch.no_grad():\n",
    "    all_hidden_states  = bert_model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "08a7347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장 임베딩을 평균하여 문장 전체 임베딩 얻기\n",
    "bert_sentence_embeddings = torch.mean(all_hidden_states, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "78d68d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.262925  ,  0.15120715,  0.06920782, ..., -0.49392438,\n",
       "         0.815937  , -0.02945545],\n",
       "       [-0.30813193, -0.07224108, -0.14033984, ..., -0.54359436,\n",
       "         0.8417747 , -0.19259213],\n",
       "       [-0.15348107,  0.13347183,  0.03283592, ..., -0.5608814 ,\n",
       "         0.69439113, -0.07183864],\n",
       "       ...,\n",
       "       [-0.35193074,  0.05294407, -0.11639063, ..., -0.61349666,\n",
       "         0.7212711 , -0.14494902],\n",
       "       [-0.2989488 , -0.06234165, -0.1374794 , ..., -0.52989614,\n",
       "         0.8737767 , -0.12753004],\n",
       "       [ 0.21697433,  0.09295134,  1.134148  , ..., -0.46585512,\n",
       "         0.06917247, -0.9795131 ]], dtype=float32)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e6e8d504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 768)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6cc55919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "d41d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 행렬 계산\n",
    "bert_similarity_matrix = cosine_similarity(bert_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "37fa0ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000002 , 0.92474496, 0.95930505, 0.95105225, 0.9318465 ,\n",
       "        0.9313162 , 0.9406705 , 0.91699517, 0.9112865 , 0.915306  ,\n",
       "        0.92282295, 0.91126865, 0.8482593 , 0.9335156 , 0.93422335,\n",
       "        0.3380583 ],\n",
       "       [0.92474496, 1.        , 0.97167194, 0.9879476 , 0.9923108 ,\n",
       "        0.99228585, 0.9898343 , 0.99011457, 0.99239063, 0.992059  ,\n",
       "        0.9910587 , 0.9920529 , 0.6996031 , 0.98957676, 0.9944438 ,\n",
       "        0.30990764],\n",
       "       [0.95930505, 0.97167194, 0.99999964, 0.97645235, 0.9691844 ,\n",
       "        0.96460795, 0.97028905, 0.95599604, 0.9562422 , 0.9575167 ,\n",
       "        0.9579422 , 0.9542211 , 0.7961651 , 0.96922976, 0.9707035 ,\n",
       "        0.34518176],\n",
       "       [0.95105225, 0.9879476 , 0.97645235, 1.0000001 , 0.9903399 ,\n",
       "        0.9885352 , 0.99288654, 0.9843228 , 0.98438823, 0.9835757 ,\n",
       "        0.98628116, 0.9843755 , 0.73644364, 0.9897797 , 0.98864293,\n",
       "        0.3204363 ],\n",
       "       [0.9318465 , 0.9923108 , 0.9691844 , 0.9903399 , 1.        ,\n",
       "        0.99639773, 0.9939338 , 0.99394107, 0.99300504, 0.9937545 ,\n",
       "        0.9953975 , 0.9934788 , 0.69015336, 0.995725  , 0.98887706,\n",
       "        0.31149364],\n",
       "       [0.9313162 , 0.99228585, 0.96460795, 0.9885352 , 0.99639773,\n",
       "        0.99999964, 0.9950137 , 0.99588776, 0.9950263 , 0.99536157,\n",
       "        0.9974062 , 0.99615955, 0.6802993 , 0.99541   , 0.9888249 ,\n",
       "        0.31046262],\n",
       "       [0.9406705 , 0.9898343 , 0.97028905, 0.99288654, 0.9939338 ,\n",
       "        0.9950137 , 1.        , 0.99361336, 0.99239707, 0.9921497 ,\n",
       "        0.99328536, 0.9924592 , 0.7017001 , 0.9939074 , 0.9891869 ,\n",
       "        0.31741238],\n",
       "       [0.91699517, 0.99011457, 0.95599604, 0.9843228 , 0.99394107,\n",
       "        0.99588776, 0.99361336, 0.99999994, 0.99853647, 0.99726   ,\n",
       "        0.9969921 , 0.9986558 , 0.65794957, 0.9924853 , 0.98749566,\n",
       "        0.29873925],\n",
       "       [0.9112865 , 0.99239063, 0.9562422 , 0.98438823, 0.99300504,\n",
       "        0.9950263 , 0.99239707, 0.99853647, 1.        , 0.9973141 ,\n",
       "        0.995404  , 0.99878395, 0.6591095 , 0.9910499 , 0.9896537 ,\n",
       "        0.30109885],\n",
       "       [0.915306  , 0.992059  , 0.9575167 , 0.9835757 , 0.9937545 ,\n",
       "        0.99536157, 0.9921497 , 0.99726   , 0.9973141 , 1.        ,\n",
       "        0.995719  , 0.997691  , 0.659509  , 0.99359936, 0.9907285 ,\n",
       "        0.29965544],\n",
       "       [0.92282295, 0.9910587 , 0.9579422 , 0.98628116, 0.9953975 ,\n",
       "        0.9974062 , 0.99328536, 0.9969921 , 0.995404  , 0.995719  ,\n",
       "        1.        , 0.99705803, 0.6649778 , 0.9940964 , 0.9871468 ,\n",
       "        0.30432293],\n",
       "       [0.91126865, 0.9920529 , 0.9542211 , 0.9843755 , 0.9934788 ,\n",
       "        0.99615955, 0.9924592 , 0.9986558 , 0.99878395, 0.997691  ,\n",
       "        0.99705803, 1.        , 0.65178096, 0.99238884, 0.98793155,\n",
       "        0.29656848],\n",
       "       [0.8482593 , 0.6996031 , 0.7961651 , 0.73644364, 0.69015336,\n",
       "        0.6802993 , 0.7017001 , 0.65794957, 0.6591095 , 0.659509  ,\n",
       "        0.6649778 , 0.65178096, 0.99999994, 0.6910321 , 0.71221364,\n",
       "        0.37546578],\n",
       "       [0.9335156 , 0.98957676, 0.96922976, 0.9897797 , 0.995725  ,\n",
       "        0.99541   , 0.9939074 , 0.9924853 , 0.9910499 , 0.99359936,\n",
       "        0.9940964 , 0.99238884, 0.6910321 , 0.9999999 , 0.98779655,\n",
       "        0.31094968],\n",
       "       [0.93422335, 0.9944438 , 0.9707035 , 0.98864293, 0.98887706,\n",
       "        0.9888249 , 0.9891869 , 0.98749566, 0.9896537 , 0.9907285 ,\n",
       "        0.9871468 , 0.98793155, 0.71221364, 0.98779655, 1.0000004 ,\n",
       "        0.30887675],\n",
       "       [0.3380583 , 0.30990764, 0.34518176, 0.3204363 , 0.31149364,\n",
       "        0.31046262, 0.31741238, 0.29873925, 0.30109885, 0.29965544,\n",
       "        0.30432293, 0.29656848, 0.37546578, 0.31094968, 0.30887675,\n",
       "        0.99999976]], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "9bf9c0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.99999964),\n",
       " (3, 0.97645235),\n",
       " (1, 0.97167194),\n",
       " (14, 0.9707035),\n",
       " (6, 0.97028905),\n",
       " (13, 0.96922976),\n",
       " (4, 0.9691844),\n",
       " (5, 0.96460795),\n",
       " (0, 0.95930505),\n",
       " (10, 0.9579422),\n",
       " (9, 0.9575167),\n",
       " (8, 0.9562422),\n",
       " (7, 0.95599604),\n",
       " (11, 0.9542211),\n",
       " (12, 0.7961651),\n",
       " (15, 0.34518176)]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 만들기, 유사도\n",
    "num_sentences = 3\n",
    "bert_summary_sentences = []\n",
    "for i in range(num_sentences):\n",
    "    sentence_scores = list(enumerate(bert_similarity_matrix[i]))\n",
    "    \n",
    "bert_sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
    "bert_sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a2e14f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the order issued on august 1 by gurpreet singh deputy secretary personnel had said.to ensure that no one skipped office an attendance report was to be sent to the government the next evening.the two notifications\n"
     ]
    }
   ],
   "source": [
    "# 요약문장 표시\n",
    "bert_summary_sentences.append(processed_article[bert_sentence_scores[1][0]])\n",
    "\n",
    "bert_summary = ' '.join(bert_summary_sentences)\n",
    "print(bert_summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "fdbd34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUGE 점수\n",
    "\n",
    "def calculate_rouge(hypothesis, reference):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "08ab4abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the administration of union territory daman and diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of rakshabandhan on august 7. the administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summary = \"The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.\"\n",
    "reference_summary = reference_summary.lower()\n",
    "reference_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "961d942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: [{'rouge-1': {'r': 0.15217391304347827, 'p': 0.21875, 'f': 0.17948717464825786}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.027777777777777776, 'f': 0.021276591018561483}, 'rouge-l': {'r': 0.15217391304347827, 'p': 0.21875, 'f': 0.17948717464825786}}]\n"
     ]
    }
   ],
   "source": [
    "# ROUGE 점수 계산\n",
    "bert_rouge_scores = calculate_rouge(bert_summary, reference_summary)\n",
    "print(\"ROUGE Scores:\", bert_rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96e058",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "140f9a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 56.5kB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:17<00:00, 31.0MB/s] \n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 15.6MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 36.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "44f55feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9c772ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = gpt_model.config.vocab_size # 모델의 vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "040aa165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  627, 9656,  ...,    0,    0,    0],\n",
       "        [   0,  405,   34,  ...,    0,    0,    0],\n",
       "        [   0,  179,   42,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,  463,  197,  ...,    0,    0,    0],\n",
       "        [   0,  627,  910,  ...,    0,    0,    0],\n",
       "        [   0,  102,   76,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "max_index = torch.max(input_ids)\n",
    "\n",
    "# input_ids 내의 모든 값을 어휘 크기 이하로 제한\n",
    "input_ids[input_ids > vocab_size] = vocab_size\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f77e26a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 118])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "18717b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30000)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = torch.max(input_ids)\n",
    "max_index # 최댓값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b729458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 출력\n",
    "with torch.no_grad():\n",
    "    all_hidden_states  = gpt_model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8258e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장 임베딩을 평균하여 문장 전체 임베딩 얻기\n",
    "gpt_sentence_embeddings = torch.mean(all_hidden_states, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6053ada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42899892, -0.3247693 ,  0.19022508, ..., -0.3396074 ,\n",
       "         0.17062797,  0.35829198],\n",
       "       [-0.13435397, -0.1554592 ,  0.67065907, ..., -0.65106493,\n",
       "        -0.00858835,  0.60235906],\n",
       "       [-0.03217074, -0.33315903,  0.65385824, ..., -0.53166044,\n",
       "         0.22086616,  0.4154809 ],\n",
       "       ...,\n",
       "       [-0.11947086, -0.0604829 ,  0.35208467, ..., -0.63463813,\n",
       "         0.08084037,  0.55952984],\n",
       "       [ 0.1426142 , -0.16174889,  0.5425769 , ..., -0.5027295 ,\n",
       "         0.23706818,  0.27046928],\n",
       "       [ 0.0072054 , -0.4489344 ,  0.5742182 , ..., -0.54602975,\n",
       "         0.17345917,  0.6082841 ]], dtype=float32)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "401b961a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 768)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e50ce43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "476b9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 행렬 계산\n",
    "gpt_similarity_matrix = cosine_similarity(gpt_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "13d90a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.8101522 , 0.9614535 , 0.9854795 , 0.7714888 ,\n",
       "        0.80390155, 0.84275997, 0.7684026 , 0.7296527 , 0.8177986 ,\n",
       "        0.722939  , 0.7646715 , 0.98207796, 0.7998913 , 0.93664646,\n",
       "        0.83389467],\n",
       "       [0.8101522 , 1.0000001 , 0.93407166, 0.8930658 , 0.9978013 ,\n",
       "        0.99977195, 0.9979865 , 0.9973528 , 0.99130106, 0.99751943,\n",
       "        0.9901617 , 0.9970695 , 0.8818018 , 0.999612  , 0.9611329 ,\n",
       "        0.9988039 ],\n",
       "       [0.9614535 , 0.93407166, 0.9999998 , 0.99305034, 0.91013896,\n",
       "        0.93054444, 0.951549  , 0.908972  , 0.8842828 , 0.94189644,\n",
       "        0.87918323, 0.90562713, 0.9761141 , 0.92731196, 0.99556315,\n",
       "        0.9482267 ],\n",
       "       [0.9854795 , 0.8930658 , 0.99305034, 0.99999964, 0.86284727,\n",
       "        0.8883541 , 0.91700935, 0.86101294, 0.8300353 , 0.90055877,\n",
       "        0.82445186, 0.85749197, 0.98825943, 0.88483906, 0.9809879 ,\n",
       "        0.911173  ],\n",
       "       [0.7714888 , 0.9978013 , 0.91013896, 0.86284727, 0.99999964,\n",
       "        0.9982228 , 0.9921088 , 0.99972165, 0.9974114 , 0.99459124,\n",
       "        0.99700344, 0.99977136, 0.8505054 , 0.99882996, 0.9419757 ,\n",
       "        0.99398804],\n",
       "       [0.80390155, 0.99977195, 0.93054444, 0.8883541 , 0.9982228 ,\n",
       "        1.        , 0.9971    , 0.9978886 , 0.9925717 , 0.9974377 ,\n",
       "        0.9914829 , 0.99769056, 0.87666184, 0.9995664 , 0.9585381 ,\n",
       "        0.99837554],\n",
       "       [0.84275997, 0.9979865 , 0.951549  , 0.91700935, 0.9921088 ,\n",
       "        0.9971    , 0.99999964, 0.9913376 , 0.98141   , 0.995427  ,\n",
       "        0.979869  , 0.99086785, 0.9084275 , 0.99679184, 0.9741852 ,\n",
       "        0.9990672 ],\n",
       "       [0.7684026 , 0.9973528 , 0.908972  , 0.86101294, 0.99972165,\n",
       "        0.9978886 , 0.9913376 , 0.9999998 , 0.9978715 , 0.994948  ,\n",
       "        0.9973992 , 0.99973834, 0.8463972 , 0.99827224, 0.9410982 ,\n",
       "        0.9932718 ],\n",
       "       [0.7296527 , 0.99130106, 0.8842828 , 0.8300353 , 0.9974114 ,\n",
       "        0.9925717 , 0.98141   , 0.9978715 , 0.9999995 , 0.988958  ,\n",
       "        0.9997704 , 0.9979744 , 0.81297237, 0.99308443, 0.9203932 ,\n",
       "        0.98513305],\n",
       "       [0.8177986 , 0.99751943, 0.94189644, 0.90055877, 0.99459124,\n",
       "        0.9974377 , 0.995427  , 0.994948  , 0.988958  , 1.0000001 ,\n",
       "        0.98723924, 0.9935435 , 0.87978303, 0.9965187 , 0.96655744,\n",
       "        0.9974584 ],\n",
       "       [0.722939  , 0.9901617 , 0.87918323, 0.82445186, 0.99700344,\n",
       "        0.9914829 , 0.979869  , 0.9973992 , 0.9997704 , 0.98723924,\n",
       "        0.9999999 , 0.9976179 , 0.8082266 , 0.9923377 , 0.91616166,\n",
       "        0.9835057 ],\n",
       "       [0.7646715 , 0.9970695 , 0.90562713, 0.85749197, 0.99977136,\n",
       "        0.99769056, 0.99086785, 0.99973834, 0.9979744 , 0.9935435 ,\n",
       "        0.9976179 , 1.0000007 , 0.8451129 , 0.9981854 , 0.9384525 ,\n",
       "        0.9927931 ],\n",
       "       [0.98207796, 0.8818018 , 0.9761141 , 0.98825943, 0.8505054 ,\n",
       "        0.87666184, 0.9084275 , 0.8463972 , 0.81297237, 0.87978303,\n",
       "        0.8082266 , 0.8451129 , 1.0000004 , 0.8743571 , 0.9655926 ,\n",
       "        0.89976954],\n",
       "       [0.7998913 , 0.999612  , 0.92731196, 0.88483906, 0.99882996,\n",
       "        0.9995664 , 0.99679184, 0.99827224, 0.99308443, 0.9965187 ,\n",
       "        0.9923377 , 0.9981854 , 0.8743571 , 0.9999999 , 0.95572287,\n",
       "        0.99775815],\n",
       "       [0.93664646, 0.9611329 , 0.99556315, 0.9809879 , 0.9419757 ,\n",
       "        0.9585381 , 0.9741852 , 0.9410982 , 0.9203932 , 0.96655744,\n",
       "        0.91616166, 0.9384525 , 0.9655926 , 0.95572287, 1.0000001 ,\n",
       "        0.9718259 ],\n",
       "       [0.83389467, 0.9988039 , 0.9482267 , 0.911173  , 0.99398804,\n",
       "        0.99837554, 0.9990672 , 0.9932718 , 0.98513305, 0.9974584 ,\n",
       "        0.9835057 , 0.9927931 , 0.89976954, 0.99775815, 0.9718259 ,\n",
       "        0.99999964]], dtype=float32)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b49001b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9999998),\n",
       " (14, 0.99556315),\n",
       " (3, 0.99305034),\n",
       " (12, 0.9761141),\n",
       " (0, 0.9614535),\n",
       " (6, 0.951549),\n",
       " (15, 0.9482267),\n",
       " (9, 0.94189644),\n",
       " (1, 0.93407166),\n",
       " (5, 0.93054444),\n",
       " (13, 0.92731196),\n",
       " (4, 0.91013896),\n",
       " (7, 0.908972),\n",
       " (11, 0.90562713),\n",
       " (8, 0.8842828),\n",
       " (10, 0.87918323)]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 만들기, 유사도\n",
    "num_sentences = 3\n",
    "gpt_summary_sentences = []\n",
    "for i in range(num_sentences):\n",
    "    sentence_scores = list(enumerate(gpt_similarity_matrix[i]))\n",
    "    \n",
    "gpt_sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
    "gpt_sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4189ebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rss is the ideological parent of the ruling bjp.last year women ministers in the modi government went to the border areas to celebrate the festival with soldiers\n"
     ]
    }
   ],
   "source": [
    "# 요약문장 표시\n",
    "gpt_summary_sentences.append(processed_article[gpt_sentence_scores[1][0]])\n",
    "\n",
    "gpt_summary = ' '.join(gpt_summary_sentences)\n",
    "print(gpt_summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "38f7d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summary = \"The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.\"\n",
    "reference_summary = reference_summary.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a376da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "bab14d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: [{'rouge-1': {'r': 0.08695652173913043, 'p': 0.17391304347826086, 'f': 0.11594202454106296}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.17391304347826086, 'f': 0.11594202454106296}}]\n"
     ]
    }
   ],
   "source": [
    "# ROUGE 점수 계산\n",
    "gpt_rouge_scores = calculate_rouge(gpt_summary, reference_summary)\n",
    "print(\"ROUGE Scores:\", gpt_rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3026c02",
   "metadata": {},
   "source": [
    "### BERT 결과와 비교"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3b7b8b82",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41419525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
